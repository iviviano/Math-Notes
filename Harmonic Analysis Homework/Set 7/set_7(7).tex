\documentclass[12pt, reqno]{amsart}

%\usepackage{upgreek}
\usepackage[margin=3.5 cm]{geometry}
\usepackage{graphicx, mathabx}
\usepackage{color}
%\usepackage{subfigure}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}          %theorem without number
\newtheorem{prop}{Proposition}[section]
\newtheorem*{prop*}{Proposition}  % proposition without number
\newtheorem{coro}{Corollary}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{conj}{Conjecture}[section]
\newtheorem{obs}{Observation}[section]

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}

\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%\numberwithin{equation}

%    Absolute value notation
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\newcommand{\ud}{\mathrm{d}}

\begin{document}

\title[Math 357 - Harmonic Analysis]{Problem set no. 7 }

\begin{titlepage}
    
\maketitle

I affirm that I have adhered to the Honor Code in this assignment. Isaac Viviano
\end{titlepage}

blank
\newpage

\section*{}

% {\bf{Only the problems marked as ``{\em{mandatory problems}}'' are to be turned in as part of this week's problem set.}} Any remaining problems are {\em{recommended}}, but you do not need to turn them in. 

% For the recommended problems, I suggest to just think about a strategy, possibly jotting down some rough ideas, but only work out the details if you have extra time at your disposal. {\bf{As stated in the syllabus, you are allowed to use the results of recommended problems (even if you do not prove them) as well as any of the problems from previous sets.}} Just make sure to clearly reference them in your work. 

% \vspace{0.2 cm}
% {\underline{\bf{Mandatory problems:}}} Problems 1; 2(a) \& (c); 3(b); 4

% \vspace{0.1 cm}
% {\bf{From the above mandatory problems, you have to turn in all mandatory problems written up using TeX.}} 
% \vspace{0.1 cm}

% \vspace{0.2 cm}
% {\underline{\bf{Recommended problems:}}} Problem 2(b); Problem 3(a)

% \vspace{0.2 cm}
% {\underline{NOTE:}} Even though you do not have to do the recommended problems, you are explicitly allowed ({\tiny{and should consider to}}) use their results in your work; as with all other results you are using, just make it clear by referencing appropriately what you use; see also the section ``homework'' in the syllabus.

% \vspace{0.2 cm}
% In the following, I will use [CM] for my book manuscript and [DJB] for David J. Benson's text.

\section{Problems:} 

\begin{itemize}

\item {\bf{Problem 1 - Equidistribution Theorem for Riemann integrable functions:}} 
%In class, we proved the equidistribution theorem for continuous, periodic functions; see also Section 7.5.1 in [CM]. The goal of this problem is to extend the equidistribution theorem to arbitrary Riemann integrable function, as stated in [CM] Theorem 7.6. After all, recall that our original motivation for the equidistribution theorem was to prove that, on average, the time that a trajectory of an irrational rotation spends in an interval $I$ is asymptotically equal to the size of the interval, see Theorem 7.5. To remind yourself of the context, first re-read Section 7.5 in [CM]: Theorem 7.5 arises from Theorem 7.6 as a special case if the function at hand is an indicator function for the interval $I$, as given in ([CM]; 7.66). 

%In parts (a) -- (b), you will use an approximation argument to first extend the continuous version of the equidistribution theorem from class to Theorem 7.5. In part (c) you will subsequently use an approximation argument to pass from a single indicator function to an arbitrary Riemann integrable function.

\vspace{0.1 cm}
\begin{itemize}
\item[(a)] 

\begin{lemma} \label{onea}
Given an interval $I \subseteq (0,1)$, let $f_I$ denote the periodic version of the indicator function for $I$ given in ([CM]; 7.66). For each $\epsilon > 0$ sufficiently small, there exist functions $f_\pm^{(\epsilon)} \in \mathcal{C}_1(\mathbb{R}; \mathbb{C})$ satisfying
\begin{equation}
f_{-}^{(\epsilon)}(t) \leq f_I(t) \leq f_{+}^{(\epsilon)}(t) ~\mbox{, for all $t \in \mathbb{R}$ ,}
\end{equation}
so that
\begin{align}
\left\vert \int_0^1 \left( f_{\pm}^{(\epsilon)}(t) - f_I(t) \right) ~\ud t \right\vert \leq \epsilon ~\mbox{.}\label{area_bound}
\end{align}
\end{lemma}

\begin{proof}
    
    Let $0<\epsilon<\min\{a,1-b, \frac{b-a}{2}\}$ and define:
$$
f_{-}^{(\epsilon)}(t):=\begin{cases}
 0 & \text{if }x\in[0,a]\\
 \frac{x-a}{\epsilon} & \text{if }x\in[a,a+\epsilon]\\
 1 & \text{if }x\in[a+\epsilon,b-\epsilon] \\
\frac{b-x}{\epsilon} & \text{if }x\in[b-\epsilon,b] \\
0 & \text{if }x\in[b,1]
\end{cases}
$$
$$
f_{+}^{(\epsilon)}(t):=\begin{cases}
 0 & \text{if }x\in[0,a-\epsilon]\\
 \frac{x-a+\epsilon}{\epsilon} & \text{if }x\in[a-\epsilon,a]\\
 1 & \text{if }x\in[a,b] \\
\frac{b-x+\epsilon}{\epsilon} & \text{if }x\in[b,b+\epsilon] \\
0 & \text{if }x\in[b,1]
\end{cases}
$$

We see that $f_{+}^{(\epsilon)}$ and $f^{(\epsilon)}_{-}$ are the functions depicted in the picture. 

Using geometric evaluation, the trapezoidal areas may be found\begin{align*}
\int_{0}^{1}f^{(\epsilon)}_{+}(x)-f(x)\ dx&= \int_{0}^{1}f^{(\epsilon)}_{+}(x)\ dx-\int_{0}^{1}f(x)\ dx\\
&= (b-a)+ \epsilon-(b-a)\\
&= \epsilon\\
\int_{0}^{1}f^{(\epsilon)}_{-}(x)-f(x)\ dx&= \int_{0}^{1}f^{(\epsilon)}_{-}(x)\ dx-\int_{0}^{1}f(x)\ dx\\
&= (b-a-\epsilon)-(b-a)\\
&= -\epsilon
\end{align*}
showing (\ref{area_bound}).
\end{proof}

%%% include figure
\begin{figure}[htbp]
%\includegraphics[width= 0.5\textwidth]{./fig_approx.pdf}
\end{figure}

\vspace{0.1 cm}
\item[(b)] %Use the conclusion from part (a) to prove [CM] Theorem 7.5. To do so, given $\epsilon > 0$ arbitrary, approximate $f_I$ by the functions $f_\pm^{(\epsilon)} \in \mathcal{C}_1(\mathbb{R}; \mathbb{C})$ from part (b). Then use the version of [CM] Theorem 7.6 for continuous functions which we proved in class to prove [CM] Theorem 7.5. 

\begin{theorem} \label{et1}
    Given $\alpha\in \mathbb{R}\setminus\mathbb{Q}$, let $R_\alpha$ be the associated rotation map. For an arbitrary, fixed interval $I\subseteq[0,1)$, let $$\mathcal{N}_{n}(x;I):=\#\{k\in\{0,\ldots,n-1\}|R_{\alpha}^{k}(x)\in I\}$$Then, $$\frac{\mathcal{N}_{n}(x;I)}{n}\rightarrow |I|$$uniformly, as $n \rightarrow \infty$.
\end{theorem}

\begin{proof}
    
We formulate Theorem \ref{et1} as a special case of Theorem \ref{et2}. Let $f_I$ be the 1-periodic extension of 
$$\chi_{I}(x)=\begin{cases}1,\text{if }x\in I \\
0,\text{if }x\notin I\end{cases}$$
We may write: $$\frac{1}{n}\mathcal{N}_{n}(x,I)= \frac{1}{n}\sum_{n=0}^{n-1}f_{I}(x+k \alpha)$$and $$|I|=\int_{0}^{1}f_{I}(x)\ dx=\widehat{[f_{I}]}_{0}$$
Now, we prove Theorem \ref{et2} for the 1-periodic indicator function $f_{I}$.
\vspace*{10 pt}
Let $s_\pm^{(\epsilon)}=f_\pm^{(\epsilon)}$ and $f=f_I$. Estimate:

\begin{align}
\left| \frac{1}{n}\sum_{k=0}^{n-1}f(x+k \alpha)-\hat f_{0}\right|&= \left| \frac{1}{n}\sum_{k=0}^{n-1}f(x+k \alpha) - \frac{1}{n}\sum_{k=0}^{n-1}s_{-}^{(\epsilon)}(x+k \alpha)+\frac{1}{n}\sum_{k=0}^{n-1}s_{-}^{(\epsilon)}(x+k \alpha)-\hat f_{0}\right|\\
&\le \left| \frac{1}{n}\sum_{k=0}^{n-1}f(x+k \alpha) - \frac{1}{n}\sum_{k=0}^{n-1}s_{-}^{(\epsilon)}(x+k \alpha)\right|+\left|\frac{1}{n}\sum_{k=0}^{n-1}s_{-}^{(\epsilon)}(x+k \alpha)-\hat f_{0}\right|\\
&\le \left| \frac{1}{n}\sum_{k=0}^{n-1}f(x+k\alpha)-s_{-}^{(\epsilon)}(x+k \alpha)\right|\label{fs}
\\
&+ \left| \frac{1}{n}\sum_{k=0}^{n-1}s_{-}^{(\epsilon)}(x+k \alpha)-\widehat{[s_{-}^{(\epsilon)}]}_{0}\right|\label{sset}
\\
&+\left|\widehat{[s_{-}^{(\epsilon)}]}_{0}-\hat f_{0}\right|\label{fcs}
\end{align}

Note that (\ref{sset}) converges to 0 uniformly by the version of Theorem \ref{et2} proven in class for continuous functions. For (\ref{fcs}), Lemma \ref{onea} gives $$\left|\widehat{[s_{-}^{(\epsilon)}]}_{0}-\hat f_{0}\right|=\left|\int_{0}^{1}s_{-}^{(\epsilon)}(t)-f(t)\ dt\right|<\epsilon$$
We estimate (\ref{fs}):
\begin{align}
\left| \frac{1}{n}\sum_{k=0}^{n-1}f(x+k\alpha)-s_{-}^{(\epsilon)}(x+k \alpha)\right|&= \frac{1}{n}\sum_{k=0}^{n-1}\underbrace{f(x+k\alpha)}_{\le~s_{+}^{(\epsilon)}(x+k \alpha)}-s_{-}^{(\epsilon)}(x+k \alpha)\\
&\le \frac{1}{n}\sum_{k=0}^{n-1} s_{+}^{(\epsilon)}(x+k \alpha)-s_{-}^{(\epsilon)}(x+k \alpha)\\
&= \left|\frac{1}{n}\sum_{k=0}^{n-1} s_{+}^{(\epsilon)}(x+k \alpha)-s_{-}^{(\epsilon)}(x+k \alpha)\right|\\
&\le \left| \frac{1}{n}\sum_{k=0}^{n-1}s_{+}^{(\epsilon)}(x+k \alpha)-\widehat{[s_{-}^{(\epsilon)}]}_{0}\right|\label{first}\\
&+\left|\widehat{[s_{+}^{(\epsilon)}]}_{0}-\widehat{[s_{-}^{(\epsilon)}]}_{0}\right|\label{something}\\
&+\left| \frac{1}{n}\sum_{k=0}^{n-1}s_{+}^{(\epsilon)}(x+k \alpha)-\widehat{[s_{-}^{(\epsilon)}]}_{0}\right|\label{last}
\end{align}
Again, (\ref{first}) and (\ref{last}) go to zero uniformly in $x$ by the version of Theorem \ref{et2} proven in class for continuous functions. For (\ref{something}) gives,
\begin{align*}
\left|\widehat{[s_{+}^{(\epsilon)}]}_{0}-\widehat{[s_{-}^{(\epsilon)}]}_{0}\right|&= \left|\int_{0}^{1}s_{+}^{(\epsilon)}(t)-s_{-}^{(\epsilon)}(t)\ dt \right|\le \epsilon\\
&= \left|\int_{0}^{1}s_{+}^{(\epsilon)}(t)-f(t)\ dt+\int_{0}^{1}f(t)-s_{-}^{(\epsilon)}(t)\ dt \right|\\
&= \left|\int_{0}^{1}s_{+}^{(\epsilon)}(t)-f(t)\ dt\right|+\left|\int_{0}^{1}f(t)-s_{-}^{(\epsilon)}(t)\ dt \right|\\
&\le \epsilon+\epsilon
\end{align*}
so, we have proven Theorem \ref{et2} for 1-periodic indicator functions.
\end{proof}

\vspace{0.1 cm}
\item[(c)] 

\begin{theorem} \label{et2}
    Fix $\alpha\in \mathbb{R}-\mathbb{Q}$ and let $f\in\mathcal{R}_{1}(\mathbb{R};\mathbb{C})$ be arbitrary. The, we have the Cesaro means of the values of $f$ evaluated along the $\alpha-$finite rotational orbits converge uniformly in $x$ with $$\frac{1}{n}\sum_{k=0}^{n-1}f(x+k \alpha)\rightarrow \int_{0}^{1}f(t)\ dt=\hat f_{0}$$as $n \rightarrow \infty$.
\end{theorem}

\begin{proof}
    
We proved Theorem \ref{et2} for 1-periodic indicator functions in part (b). We begin by extension of Theorem \ref{et2} from 1-periodic indicator functions to 1-periodic real-valued step functions. We consider an arbitrary step function $f$ defined by $$f=\sum_{I\in A}a_{I}f_{I}$$where $A$ is a finite collection of intervals and $a_{I}\in\mathbb{R}$ for all $I\in A$, and $f_{I}$ denotes the 1-periodic extension of the indicator $\chi_{I}$. For any $x\in \mathbb{R}$,
\begin{align}
    \left| \frac{1}{n}\sum_{k=0}^{n-1}f(x+k\alpha)-\hat{f}_0\right|&= \left| \frac{1}{n}\sum_{k=0}^{n-1}\sum_{I\in A}a_{I}f_{I}(x+k \alpha)-\widehat{\left[\sum_{I\in A}a_{I}f_{I}\right]}_{0}\right|\\
    &= \left| \sum_{I\in A}a_{I} \frac{1}{n}\sum_{k=0}^{n-1}f_{I}(x+k \alpha)-\sum_{I\in A}a_{I}\widehat{\left[f_{I}\right]}_{0}\right|\\
&= \left| \sum_{I\in A}a_{I} \left(\frac{1}{n}\sum_{k=0}^{n-1}f_{I}(x+k \alpha)-\widehat{\left[f_{I}\right]}_{0}\right)\right| \label{step_ex}
\end{align}
where each term of the finite sum in (\ref{step_ex}) converges to 0 uniformly in $x$. Therefore, the Equidistribution Theorem holds for all real-valued 1-periodic step functions.

\vspace*{10 pt}
We next extend from 1-periodic step functions to real-valued Riemann integrable functions. Let $f\in\mathcal{R}_1(\mathbb{R;R})$ and $\epsilon>0$ be arbitrary. 


\vspace*{10 pt}

First, we show there exist 1-periodic step functions $s_{\pm}^{(\epsilon)}$ such that $s^{(\epsilon)}_{-}\le f\le s_{+}^{(\epsilon)}$ and 
\begin{equation} \label{s_bound}
    \left|\int_{0}^{1} \left(s_{\pm}^{(\epsilon)}(t)-f(t)\right)\ dt\right|\le \epsilon
\end{equation}

\vspace*{10 pt}

We first observe that for any partition of $[0,1]$, the upper and lower Riemann sums may be written as the integrals of step functions. By the definition of Riemann integrable, pick a partition $x_{0}<\cdots<x_{n}$ of $[0,1]$ such that 
 $$\sum_{i=0}^{n-1} (M_{i}-m_{i})(x_{i+1}-x_{i})<\epsilon$$
where \begin{align*}
M_{i}&:= \sup_{x\in[x_{i},x_{i+1})}f(x),~0\le i<n\\
m_{i}&:= \inf_{x\in[x_{i},x_{i+1})}f(x),~0\le i<n
\end{align*}
Note: use a closed interval for the final $i=n-1$ case. Define two step functions \begin{align*}
s^{(\epsilon)}_{+}(x)&:= \sum_{i=0}^{n-1}M_{i}\chi_{[x_{i},x_{i+1})}(x)\ge f(x)\\
s^{(\epsilon)}_{-}(x)&:= \sum_{i=0}^{n-1}m_{i}\chi_{[x_{i},x_{i+1})}(x)\le f(x)\\
\end{align*}
Observe that these step functions integrate to the Riemann sums: \begin{align*}
\int_{0}^{1}s_{+}^{(\epsilon)}(x)\ dx&= \sum_{i=0}^{n-1}M_{i}(x_{i+1}-x_{i})\\
\int_{0}^{1}s_{-}^{(\epsilon)}(x)\ dx&= \sum_{i=0}^{n-1}m_{i}(x_{i+1}-x_{i})\\
\end{align*}
Additionally, we have $s_-^{(\epsilon)}\le f\le s_+^{(\epsilon)}$. For all $x\in[0,1]$, we have $x_i\le x<x_{i+1}$ for some $i$ or $x_{n-1}\le x\le x_n$. In the first case,
\[
    s_-^{(\epsilon)}(x)=m_i\le f(x)\le M_i=s_+^{(\epsilon)}(x)
\]  
and in the second case,
\[
    s_-^{(\epsilon)}(x)=m_{n-1}\le f(x)\le M_{n-1}=s_+^{(\epsilon)}(x)
\]

For the desired bound in (\ref{s_bound}):
\begin{align*}
0\le\int_{0}^{1}s_{+}^{(\epsilon)}(t)-\underbrace{f(t)}_{\ge s_{-}^{(\epsilon)}(t)}\ dt&\le \int_{0}^{1}s^{(\epsilon)}_{+}(t)\ dt-\int_{0}^{1}s^{(\epsilon)}_{-}(t)\ dt\\
&= \sum_{i=0}^{n-1}M_{i}(x_{i+1}-x_{i})-\sum_{i=0}^{n-1}m_{i}(x_{i+1}-x_{i})\\
&= \sum_{i=0}^{n-1}(M_{i}-m_{i})(x_{i+1}-x_{i})\\
&< \epsilon\\
0\le\int_{0}^{1}\underbrace{f(t)}_{\le s_{+}^{(\epsilon)}(t)}-s_-^{(\epsilon)}(t)\ dt&\le \int_{0}^{1}s^{(\epsilon)}_{+}(t)\ dt-\int_{0}^{1}s^{(\epsilon)}_{-}(t)\ dt< \epsilon\\
\end{align*}
Giving $$\left|\int_{0}^{1}s_{\pm}^{(\epsilon)}(t)-f(t)~dt\right|<\epsilon$$

Observe that this is the same exact set up as part (b), since we have Theorem \ref{et2} for $s^{(\epsilon)}_\pm$. Thus, we have proven Theorem \ref{et2} for real-valued Riemann integrable functions $f$. 

\vspace*{10 pt}

Extension to complex valued functions:
Let $f\in\mathcal{R}_1(\mathbb{R};\mathbb{C})$ be defined by $$f=h+ig$$for $h,g\in\mathcal{R}_1(\mathbb{R};\mathbb{R})$.

\begin{align}
\left| \frac{1}{n}\sum_{k=0}^{n-1}f(x+k \alpha)-\hat f_{0}\right|&= \left| \frac{1}{n}\sum_{k=0}^{n-1}(h(x+k \alpha)+ig(x+k \alpha))-\widehat{[h-ig]}_{0}\right|\\
&= \left| \frac{1}{n}\sum_{k=0}^{n-1}h(x+k \alpha)-\hat h_{0}+ \frac{1}{n}\sum_{k=0}^{n-1}ig(x+k \alpha)-i\hat g_{0}\right|\\
&\le \left| \frac{1}{n}\sum_{k=0}^{n-1}h(x+k \alpha)-\hat h_{0}\right|+\left| \frac{1}{n}\sum_{k=0}^{n-1}g(x+k \alpha)-\hat g_{0}\right| \label{com_ex}
\end{align}
where both terms of (\ref{com_ex}) converge to $0$ uniformly in $x$ as $n \rightarrow \infty$ by the Equidistribution Theorem for real-valued Riemann integrable functions.
\end{proof}

% \begin{itemize}
% \item First use your conclusions from part (b) to prove [CM] Theorem 7.6 for all periodic {\em{step functions}} (this should be quick); for the definition of step functions, see problem 1 of set 4. 
% \item Now, extend the validity from step functions to all Riemann integrable functions. To this end, use the definition of Riemann integrability via upper and lower Riemann sums to show that for a each given $f \in \mathcal{R}_1(\mathbb{R}; \mathbb{C})$ and each $\epsilon > 0$, there exist 1-periodic step functions $s_\pm^{(\epsilon)}$ such that $s_-^{(\epsilon)} \leq f \leq s_+^{(\epsilon)}$ and 
% \begin{align}
% \left\vert \int_0^1 \left( s_{\pm}^{(\epsilon)}(t) - f(t) \right) ~\ud t \right\vert \leq \epsilon ~\mbox{.}
% \end{align}
% Now use this to finish the proof of [CM] Theorem 7.6 for an arbitrary function $f \in \mathcal{R}_1(\mathbb{R}; \mathbb{C})$. 
% \end{itemize}
\end{itemize}

\vspace{0.2 cm}

\item {\bf{Problem 2 - Rate of Convergence and Diophantine Conditions:}} 

\vspace{0.1 cm}
\begin{itemize}
% \item[(a)] Read Section 7.5.2 in [CM] which discusses how a Diophantine condition can be used to overcome the small divisor problem in solving the cohomological equation. {\em{There is nothing to turn in here.}} 

% \vspace{0.1 cm}
% \item[(b)] Proposition 7.5.2 illustrates that in general, when trying to solve the cohomological equation, a condition on $\alpha$ is {\em{necessary}}. Use the strategy outlined following Remark 7.9 to prove this proposition.

\vspace{0.1 cm}
\item[(c)] %Remark 7.9 mentions that Proposition 7.5.2 implicitly shows the existence of irrational number which are {\em{not}} Diophantine. Supply the details for this remark, using the following idea:

\begin{prop}
    There exists an irrational number $\alpha$ such that $\alpha\notin\mathcal{DC}$.
\end{prop}

\begin{proof}
    
    Let $\phi:[0,\infty)\rightarrow [0,\infty)$ be defined by $$\phi(x):=e^{x}$$
    Define $$f_{n}(t):=e^{-|n|}~e^{2\pi int},~t\in \mathbb{R}$$
    Note that
    $$\|f_{n}\|_{\infty}=e^{-|n|}$$
    
    
    At $t=0$, $f_{n}(t)=e^{-|n|}$, for all $n\in\mathbb{Z}\setminus\{0\}$. So, $$\sum_{0\ne n=-\infty}^{\infty} \|f_{n}\|_{\infty}=\sum_{n=1}^{\infty}\|f_{-n}\|_{\infty}+\sum_{n=1}^{\infty}\|f_{n}\|_{\infty}=\sum_{n=1}^{\infty}e^{-n}+\sum_{n=1}^{\infty}e^{-n}$$
    That $e^{-n}$ is summable follows from the comparison test. Since $\log$ is monotonic, 
    \begin{align*}
    &e^{-n}\le \frac{1}{n^{2}}\\
    &\iff n^{2}\le~e^{-n}\\
    &\iff n^{2}\le e^{n}\\
    &\iff \log n^{2}\le \log e^{n}\\
    &\iff 2\log n\le n
    \end{align*}
    Since $\sqrt{}$ is also a monotonic function, if $n\ge16$, $$2\log n=4\log \sqrt{n}\le \sqrt{n}\sqrt{n}=n$$
    So, we see that $$e^{-n}\le \frac{1}{n^{2}}$$for all $n\ge 16$. Since $e^{-n}=|e^{-n}|$, and $\frac{1}{n^{2}}$ is a summable $p$-series we see that $e^{-n}$ is absolutely summable and thus summable. So, $$\sum_{0\ne n=-\infty}^{\infty}f_{n}$$converges uniformly in $x$ to a function $$f:=\sum_{0\ne n=-\infty}^{\infty}f_{n}$$
    
    We use induction to show that $f\in\mathcal{C}^{\infty}$. The base case was just shown. Suppose that for some $k\ge0$, $f$ is $k$ times continuously differentiable with derivative $$\frac{d^{k}}{dx^{k}}f=f^{(k)}=\sum_{0\ne n=-\infty}^{\infty}f^{(k)}_{n}$$Note that this is equivalent to $f\in\mathcal{C}^{k}$. 
    We have that $$\sum_{0\ne n=-\infty}^{\infty}f_{n}^{(k)}$$converges point-wise everywhere per the inductive hypothesis.
    
    We have $$f_{n}^{(k)}(t)=(2\pi i|n|)^{k}e^{-|n|}~e^{2\pi int}$$
    For any closed interval $I$, $$\|f^{(k)}\|_{\infty;I}\le \|f^{(k)}\|_{\infty}=(2\pi |n|)^{k}e^{-|n|}$$
    $$\sum_{0\ne n=-\infty}^{\infty} \|f^{(k)}_{n}\|_{\infty;I}=\sum_{n=1}^{\infty}\|f^{(k)}_{-n}\|_{\infty;I}+\sum_{n=1}^{\infty}\|f^{(k)}_{n}\|_{\infty;I}$$
    Again, we use the comparison test to show that $f'$ is norm summable: 
    \begin{align*}
    &(2\pi|n|)^{k}e^{-|n|}\le \frac{1}{|n|^{2}}\\
    &\iff (2\pi)^{k}|n|^{k+2}\le e^{|n|}\\
    &\iff k\log2\pi+(k+2)\log|n|\le |n|
    \end{align*}
    Note that $\log 2\pi<\log|n|$ if $|n|\ge 2\pi$. Pick an integer $N>2\pi,(4k+2)^{4}$. Then, if $|n|\ge N$, 
\begin{align*}
    k\log 2\pi+(k+2)\log |n|&\le (2k+2)\log|n|\\
    &= (4k+4)\log \sqrt{|n|}\\
    &\le \sqrt{|n|}\log \sqrt{|n|}\\
    &\le \sqrt{|n|}\sqrt{|n|}\\
    &= |n|
    \end{align*}
    So, we see that $$\|f^{(k)}_{n}\|_{\infty;I}\le|n|^{2}$$for all $n\ge N$. Thus, the comparison test implits that the double sided sequence of functions $f_{n}^{(k)}$ is norm summable under the supremum norm.
    
    We have verified both hypotheses of Theorem \ref{thm_seriesderiv}. Thus, $$f^{(k)}:=\sum_{0\ne n=-\infty }^{\infty}f^{(k)}_{n}$$is a $\mathcal{C}^{1}$ function with $$f^{(k+1)}=\sum_{0\ne n=-\infty}^{\infty}f^{(k+1)}_{n}$$
    Additionally, $f\in\mathcal{C}^{k+1}$.
    
    By (PMI), we have $f\in\mathcal{C}^k$ for all $k\in \mathbb{N}_{0}$ and thus, $$f\in\mathcal{C}^{\infty}=\bigcap_{k=0}^{\infty}\mathcal{C}^{k}$$Consider the cohomological equation: 
    \begin{equation}
    f(x)-\hat f_{0}=h(x+ \alpha)-h(x)\label{cohom}
    \end{equation}
    We showed that $\{e^{-n}\}_{n\in \mathbb{N}}$ is summable. 
    Thus, Propostion 7.5.2 of [CM] implies that there exists a continuous function and an irrational $\alpha$ such that the cohomological equation (\ref*{cohom}) has no continuous solution $h$. We also note from the convergent Fourier series of $f$, that it has the same Fourier coefficients as the function given by Proposition 7.5.2. Thus, the uniqueness property of continuous functions implies that for the $f$ and $\alpha$ chosen, there is no continuous solution $h$ to (\ref{cohom}).
    
    \vspace*{10 pt}
    If $\alpha$ were Diophantine for some $r>0$, then Proposition 7.5.1 of [CM] would imply that for all $g\in\mathcal{C}_{1}^{l}(\mathbb{R};\mathbb{C})$ with $l>r+1$, the cohomological equation (\ref{cohom}) would have a continuous solution. That $f\in\mathcal{C}_{1}^{\infty}(\mathbb{R};\mathbb{C})\subseteq\mathcal{C}^{l}_{1}(\mathbb{R};\mathbb{C})$ with no continuous solution to (\ref{cohom}) implies that $\alpha\notin\mathcal{DC}(r)$ for any $r>0$. In particular, we have found $\alpha\notin\mathcal{DC}$.
\end{proof}

% \vspace{0.1 cm}
% {\underline{Hint:}} Show that by taking $\phi(x) = \mathrm{e}^{x}$, the associated function according to Proposition 7.5.2,
% \begin{equation}
% f(t) = \sum_{0 \neq n= -\infty}^{+\infty} \mathrm{e}^{-n} ~\mathrm{e}^{2 \pi i n t} ~\mbox{, } t \in \mathbb{R} ~\mbox{,}
% \end{equation}
% is a $\mathcal{C}^\infty$-function. To do so, use Theorem \ref{thm_seriesderiv} from problem 3 to justify ``interchanging differentiation and infinite sums;'' make sure to carefully check all hypotheses of Theorem \ref{thm_seriesderiv} ({\em{no heuristic arguments, no sweeping things under the rug!}}).
\end{itemize}

\vspace{0.2 cm}
\item {\bf{Problem 3 - Interchanging Derivatives and Infinite Sums:}} %In this problem you will prove a theorem (Theorem \ref{thm_seriesderiv}) which allows to justify ``interchanging of derivatives and infinite sums.'' You needed this result in problem 2(c).

% First, recall the following result about interchanging limits and derivatives for sequences functions, which you proved in Math 301:
% \begin{theorem}[``{\bf{interchanging limits and derivatives}}''] \label{thm_Math301_limitsderiv}
% Let $I \subseteq \mathbb{R}$ be an open interval, and for n $\in \mathbb{R}$, let $f_n \in \mathcal{C}^1(I)$. Suppose that {\bf{both}} of the following conditions hold:
% \begin{itemize}
% \item[(i)] For some function $g: I \to \mathbb{R}$, one has that $f_n^\prime \to g$ uniformly on compact subsets of $I$,
% \item[(ii)] For some $x_0 \in I$, the limit $\lim_{n \to \infty} f_n(x_0)$ exists.
% \end{itemize}

% Then, $f_n \to f$ uniformly on compact subsets of $I$, for some function $f: I \to \mathbb{R}$, satisfying $f^\prime = g$. In short, in this situation, we have that
% \begin{equation*}
% \left( \lim_{n \to \infty} f_n \right)^\prime = \lim_{n \to \infty} f_n^\prime ~\mbox{.}
% \end{equation*}
%\end{theorem}

\vspace{0.1 cm}
\begin{itemize}
%\item[(a)] In relation to Theorem \ref{thm_Math301_limitsderiv}, note that without additional hypotheses (e.g., such as stated in Theorem \ref{thm_Math301_limitsderiv}), uniform convergence does in general {\em{not}} preserve differentiability: give an example of a sequence of periodic $\mathcal{C}^\infty$-functions which uniformly convergences to a {\em{merely}} continuous function. {\em{Remember, that any example must include a proof which demonstrates the validity of the example.}}

\vspace{0.1 cm}
\item[(b)] %Use Theorem \ref{thm_Math301_limitsderiv} to prove the following useful result:
\begin{theorem}[``Interchanging Derivatives and Series''] \label{thm_seriesderiv}
Given and open interval $I \subseteq \mathbb{R}$ and $\mathcal{C}^1$-functions $f_n: I \to \mathbb{R}$, $n \in \mathbb{N}$. Suppose that {\bf{both}} of the following conditions hold:
\begin{itemize}
\item[(i)] The series $\sum_{n=1}^{\infty} f_n(x)$ converges for at least one point $x \in I$.
\item[(ii)] For all closed sub-intervalls $[a,b] \subseteq I$, we have that 
\begin{equation*}
0 \leq \sum_{n=1}^{\infty} \Vert f_n^\prime \Vert_{\infty; [a,b]} < +\infty ~\mbox{;}\label{norm_bound}
\end{equation*}
here, we denote as usual
\begin{equation}
\Vert f^\prime \Vert_{\infty; [a,b]} := \sup_{x \in [a,b]} \vert f^\prime(x) \vert
\end{equation}
\end{itemize}
Then, $f(x) = \sum_{n=1}^{\infty} f_n(x)$ converges uniformly on all compact subsets of $I$ (and, thus, (absolutely) for all $x \in I$) and one has that
\begin{equation*}
\dfrac{\ud }{\ud x} \sum_{n=1}^\infty f_n(x) = \sum_{n=1}^\infty \dfrac{\ud f_n}{\ud x} ~\mbox{.}
\end{equation*}
\end{theorem}

\begin{proof}
    
    Let $f_n$ be a sequence satisfying (i) and (ii) of \ref{thm_seriesderiv} on an open interval $I$. 


Note that any compact subset of $\mathbb{R}$ has a minimum and maximum by (EVT). For any compact subset $K$ of $I$, 
$$
K\subseteq[\min K,\max K]:=[a,b]
$$
Also,
$$
\|f'_{n}\|_{\infty;K}\le\|f'_{n}\|_{\infty'[a,b]}=:M_{n}
$$ By the second hypothesis (\ref{norm_bound}), the series $$\sum_{n=1}^{\infty}M_{n}<\infty$$So, the series $$
\sum_{n=1}^{\infty}f_{n}'
$$converges uniformly to $g\in\mathcal{B}(I,\mathbb{C})$ by the Weierstrass $M$-Test. 

Letting $$
\{g_{n}\}_{n\in \mathbb{N}_{0}}:=\left\{ \sum_{k=1}^{n}f_{n}\right\}_{n\in \mathbb{N}_{0}}
$$
we see that, the sequence $$
\{g_{n}'\}_{n\in \mathbb{N}_{0}}=\left\{  \frac{d}{dx}\sum_{k=1}^{n}f_{n}\right\}_{n\in \mathbb{N}_{0}}=\left\{ \sum_{k=1}^{n}f_{n}'\right\}_{n\in \mathbb{N}_{0}}
$$converges uniformly to $g$ on all compact subsets of $I$. 

By the first hypothesis, we have 
\begin{equation}
g_{n}(x)=\sum_{k=1}^{n}f_{n}(x)\text{ converges for some }x\in I\label{con}
\end{equation}
Thus, the sequence $g_{n}$ satisfies both conditions of the "interchanging limits and derivatives" theorem. So, $g_{n}\rightarrow f$ uniformly on compact subsets of $I$ for some function $f:I \rightarrow \mathbb{R}$ satisfying $f'=g$. We see that $f=\sum_{n=1}^{\infty}f_{n}$ from (\ref{con}). And, $$
\frac{d}{dx}\sum_{n=1}^{\infty}f_{n}=f'=g=\sum_{n=1}^{\infty}f_{n}'=\sum_{n=1}^{\infty} \frac{df_n}{dx}
$$
    

\end{proof}


\end{itemize}

% {\bf{The following problem will serve as an important preparation for our upcoming discussion of the Fourier transform}}.

\vspace{0.2 cm}
\item {\bf{Problem 4 - Schwartz functions:}}

\vspace{0.1 cm}
\begin{itemize}
\item[(a)] %A complex valued function $f: \mathbb{R} \to \mathbb{C}$ is called a {\bf{Schwartz function}} (write $f \in \mathcal{S}(\mathbb{R}; \mathbb{C})$) if for all $m,n \in \mathbb{N}_0$, one has
% \begin{equation} \label{eq_defschwartz}
% C_{m,n}:= \sup_{x \in \mathbb{R}} \vert x^m f^{(n)}(x) \vert < \infty ~\mbox{.}
% \end{equation}
% Here, $f^{(n)}$ denotes the $n$-th derivative of $f$ with the convention that $f^{(0)} := f$. Intuitively a Schwartz function $f$ has the property that {\em{$f$ and all its derivatives decay faster than any polynomial.}} 


\begin{prop}
    $\mathcal{S}(\mathbb{R;C})$ is a vector subspace of $\mathcal{C}^\infty$.
\end{prop}

\begin{proof}

    Clealy, 
    \[
        \mathcal{S}(\mathbb{R;C})\subseteq\mathcal{C}^\infty
    \]

    Let $f,g\in\mathcal{S}(\mathbb{R};\mathbb{C})$ and $a,b\in \mathbb{C}$ and $m,n\in \mathbb{N}_{0}$. Let $C_{m,n}$ and $D_{m,n}$ be the Schwartz constants of $f$ and $g$, respectively. We see: 
    
    \begin{align}
\sup_{x\in\mathbb{R}}\left|x^{m}(af^{(n)}(x)+bg^{(n)}(x))\right|&= \sup_{x\in\mathbb{R}}\left|ax^{m}f^{(n)}(x)+bx^{m}g^{(n)}(x))\right|\\
&\le \sup_{x\in\mathbb{R}}a\left|x^{m}(f^{(n)}(x)\right|+b\sup_{x\in\mathbb{R}}\left|g^{(n)}(x))\right| \label{te_sup}\\
\\&= aC_{m,n}+bD_{m,n}\\
&< \infty
\end{align}

so, $af+bg\in\mathcal{S}(\mathbb{R};\mathbb{C})$. Note that (\ref{te_sup}) uses the triangle inequality and positive homogeneity for the supremum norm. 

\vspace*{10 pt}

Clearly, $0\in\mathcal{S}(\mathbb{R};\mathbb{C})$ with $C_{m,n}=0$ for all $m,n\in \mathbb{N}_{0}$. Therefore, $\mathcal{S}(\mathbb{R;C})$ is a vector subspace of $\mathcal{C}^\infty(\mathbb{R;C})$.
    
\end{proof}

\begin{prop}
    $\mathcal{S}(\mathbb{R;C})$ is closed under multiplication: 

\begin{equation}
f,g \in \mathcal{S}(\mathbb{R}; \mathbb{C}) \Longrightarrow f \cdot g \in \mathcal{S}(\mathbb{R}; \mathbb{C}) ~\mbox{.}
\end{equation}
\end{prop}

\begin{proof}
    Repeated applications of the product rule show:
\begin{align*}
(f\cdot g)^{(n)}=\sum_{k=0}^{n}\binom{n}{k}f^{(k)}g^{(n-k)}\\
\end{align*}
\begin{align}
\sup_{x\in \mathbb{R}}\left|x^{m}(f\cdot g)^{(n)}(x)\right|&= \sup_{x\in \mathbb{R}}\left|x^{m}\sum_{k=0}^{n}\binom{n}{k}f^{(k)}(x)g^{(n-k)}(x)\right|\\
&\le \sum_{k=0}^{n}\binom{n}{k}\sup_{x\in \mathbb{R}}\left|x^{m}f^{(k)}(x)g^{(n-k)}(x)\right|\\
&\le \sum_{k=0}^{n}\binom{n}{k}\sup_{x\in \mathbb{R}}\left|x^{m}f^{(k)}(x)\right|\cdot\sup_{x\in \mathbb{R}}\left|g^{(n-k)}(x)\right|\\
&= \sum_{k=0}^{n}\binom{n}{k}C_{m,k}\cdot D_{0,n-k}\label{finite}\\
&< \infty 
\end{align}
where (\ref{finite}) is finite since it is a finite sum.
\end{proof}


\begin{prop}
If $f \in \mathcal{S}(\mathbb{R}; \mathbb{C})$, then also all its derivatives satisfy
\begin{equation}
f^{(n)} \in \mathcal{S}(\mathbb{R}; \mathbb{C}) ~\mbox{, for all } n \in \mathbb{N} ~\mbox{.}
\end{equation}
\end{prop}

\begin{proof}
    Let $f\in\mathcal{S}(\mathbb{R};\mathbb{C})$ have Schwartz constants $C_{m,n}$. For a fixed $k\in \mathbb{N}$, and any $n,m\in \mathbb{N}_{0}$, let $[f^{(k)}]^{(n)}$ denote the $n$-th derivative of the $k$-th derivative of $f$. We have $$\sup_{x\in \mathbb{R}}\left|x^{m}[f^{(k)}]^{(n)}\right|=\sup_{x\in \mathbb{R}}\left|x^{m}f^{(k+n)}(x)\right|=C_{m,n+k}<\infty$$showing that $f^{(k)}\in\mathcal{S}(\mathbb{R};\mathbb{C})$.
\end{proof}

\vspace{0.1 cm}
\item[(b)] 

\begin{prop}
The Gaussian, 
\begin{equation} \label{eq_gaussian}
f: \mathbb{R} \to \mathbb{R} ~\mbox{, } f(x) = \mathrm{e}^{-\pi x^2}
\end{equation}
is a Schwartz function.
\end{prop}

\begin{proof}
    
    Let $P(k)$ be that the $k$-th derivative of $f$ is of the form $$
f^{(k)}(x)=p_{k}(x)e^{-\pi x^{2}}
$$for some polynomial $p_{k}$. \\

Base case: $k=0$
Taking $p_{0}(x)=1$, we see that $P(0)$ holds. \\

Inductive Step: Suppose for some $k\ge0$, $P(k)$ holds for a polynomial $p_{k}(x)=\sum_{i=0}^{n_{k}}a_{i}x^{i}$. 

We compute the $(k+1)$-st derivative of $f$:
\begin{align*}
\frac{d}{dx}\left(p_{k}(x)e^{-\pi x^{2}}\right)&= \frac{d}{dx}\sum_{i=0}^{n_{k}}a_{i}x^{i}e^{-\pi x^{2}}\\
&= \sum_{i=0}^{n_{k}} \frac{d}{dx}\left(a_{i}x^{i}e^{-\pi x^{2}}\right)\\
&= \sum_{i=0}^{n_{k}}ia_{i}x^{i}e^{-\pi x^{2}}-2a_{i}x^{i+1}e^{-\pi x^{2}}\\
&= \left(\sum_{i=0}^{n_{k}}ia_{i}x^{i}-2a_{i}x^{i+1}\right)e^{-\pi x^{2}}\\
&:= p_{k+1}(x)e^{-\pi x^{2}}
\end{align*}
showing $P(k+1)$.\\

Thus, by (PMI), $P(k)$ holds for all $k\in \mathbb{N}_{0}$. 
\\


Fix $n,m\in \mathbb{N}_{0}$ and examine the $n$-th derivative of $f$: $$
f^{(n)}(x)=p_{n}(x)e^{-\pi x^{2}}
$$with $p_{n}$ degree $k$.

\vspace*{10 pt}

Pick $x_{0}\in \mathbb{R}$ such that for for all $x\notin[-x_{0},x_{0}]$, 
\begin{equation} \label{polybound}
    |p_{n}(x)|\le |x^{k+1}|
\end{equation}
and $x_{0}\ge m+k+1$. Note that we can do (\ref{polybound}) since for all $|x|\ge1$, 
\begin{align*}
\left|\sum_{i=0}^{N}a_{i}x^{i}\right|&\le \sum_{i=0}^{N}|a_{i}|\cdot|x|^{i}\\
&\le \sum_{i=0}^{N}|a_{i}|\cdot|x|^{N}\\
&= |x|^{N}\sum_{i=0}^{N}|a_{i}|
\end{align*}
So, if we also require $|x|\ge\sum_{i=0}^{N}|a_{i}|$, (\ref{polybound}) holds.

We have:
\[
    |x^m f^{(n)}(x)|=|x^m p_n(x)e^{-\pi x^2}|\le|x|^{m+k+1}e^{-\pi x^{2}}
\]
Since $\log$ is monotonic, we have 
\begin{align*}
&|x|^{m+k+1}e^{-\pi x^{2}} \le  1\\
&\iff |x|^{m+k+1}\le e^{\pi x^{2}}\\
&\iff\log |x|^{m+k+1}\le \log e^{\pi x^{2}}\\
&\iff(m+k+1)\log |x|\le \pi x^{2}
\end{align*}
For a fixed $x\notin[-x_{0},x_{0}]$, $|x|\ge m+k+1$ and $\log |x|\le |x|$. So we have 
\begin{align}
(m+k+1)\log |x|&\le (m+k+1)\cdot|x|\\
&\le (m+k+1)^{2}\\
&\le |x|^{2} \label{mono}
\\&\le \pi x^{2}
\end{align}
where (\ref{mono}) is true because $y^{2}\le z^{2}\iff y\le z$ for all $y,z\in \mathbb{R}$. So, we have indeed have that 
\[
    |x|^{m+k+1}e^{-\pi x^{2}}\le1
\]
for all $x\notin[-x_{0},x_{0}]$.

Then,
\begin{align*}
\sup_{x\notin [-x_{0},x_{0}]}\left|x^{m}f^{(n)}(x)\right|&\le\sup_{x\notin [-x_{0},x_{0}]}\left|x^{m+k+1}e^{-\pi x^{2}}\right|\le1
\end{align*}
We also have the continuous function $x^{m}f^{(n)}(x)$ is bounded on the compact domain $[-x_{0},x_{0}]$ by the (EVT). Therefore, there exists 
\[
C_{m,n}:=\sup_{x\in \mathbb{R}}\left|x^{m}f^{(n)}(x)\right|
\]
\end{proof}

\end{itemize}




\end{itemize}

\end{document}

%------------------------------------------------------------------------------
% End of journal.tex
%------------------------------------------------------------------------------
